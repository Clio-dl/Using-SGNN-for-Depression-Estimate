{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_graph_4_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12E4X805kdAP-Gt1WUenTeNurpIBRUJxu",
      "authorship_tag": "ABX9TyOw0fAX6yBugqkJCJc02Ey7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sword-ace/Using-SGNN-for-Depression-Estimate/blob/main/build_graph_4_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXo_S28Vvf8M"
      },
      "source": [
        "# !pip install word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8p1HjVT_Nok"
      },
      "source": [
        "# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu113.html\n",
        "# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu113.html\n",
        "# !pip install -q torch-geometric "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf5F27NPv3fG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec0934b-705c-4bf1-a961-75e477a38bc4"
      },
      "source": [
        "# from transformers import BertModel, BertTokenizer, XLNetModel, XLNetTokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import torch\n",
        "stops = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "RzpGzP3QBi7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuwIidNgwRtN"
      },
      "source": [
        "\n",
        "#Text cleaning function\n",
        "def preprocessingText(text):\n",
        "  # text = text.lower() #text to lowercase\n",
        "  # text = re.sub(r'&lt;', '', text) #remove '&lt;' tag\n",
        "  # text = re.sub(r'<.*?>', '', text) #remove html\n",
        "  # text = re.sub(r'[0-9]+', '', text) #remove number\n",
        "  # text = \" \".join([word for word in text.split() if word not in stops]) #remove stopwords\n",
        "  # text = re.sub(r'[^\\w\\s]', '', text) #remove punctiation\n",
        "  # text = re.sub(r'[^\\x00-\\x7f]', '', text) #remove non ASCII strings\n",
        "  # text = re.sub('\\[.*?\\]', '', text)\n",
        "  # text = re.sub(r'http', '', text)\n",
        "  # text = re.sub(r'www', '', text)\n",
        "  # text = re.sub(r'https', '', text)\n",
        "  # text = re.sub('<.*?>+', '', text)\n",
        "  # text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "  # text = re.sub('\\n', '', text)\n",
        "  # text = re.sub('\\w*\\d\\w*', '', text)\n",
        "  for c in ['\\r', '\\n', '\\t'] :\n",
        "    text = re.sub(c, ' ', text) #replace newline and tab with tabs\n",
        "  # text = re.sub('\\s+', ' ', text) #replace multiple spaces with one space\n",
        "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnBtRtkVrIsM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkg8u-9hqjRE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZozMYxI0w4O"
      },
      "source": [
        "# import torch\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "from torch import Tensor as Tensor\n",
        "from torch.nn import Linear as Linear\n",
        "import torch.nn.init as init\n",
        "from torch.nn.init import _calculate_correct_fan, calculate_gain\n",
        "import torch.nn.functional as F\n",
        "# from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric import data\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR9H7vAqqIYa"
      },
      "source": [
        "\n",
        "# ###this is for loading your text file \n",
        "# data_train = pd.read_pickle(file_address) \n",
        "\n",
        "# text_train = data_train.text\n",
        "\n",
        "# preprocessed_text_train = [preprocessingText(text) for text in text_train]\n",
        "# # print(data_train.Labels.values)\n",
        "\n",
        "# y_train = torch.from_numpy(np.vstack(data_train.Labels.values)).float()\n",
        "\n",
        "# print('AFTER CLEANING: {}'. format(preprocessed_text_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fKSbYUDS5p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8P6pKp34GqT",
        "outputId": "f88d451d-08ed-4bf2-bb5f-0f406b706959"
      },
      "source": [
        "use_cuda = True\n",
        "gpu = 0\n",
        "if use_cuda:\n",
        "      device = torch.device(\"cuda:\" + str(gpu))\n",
        "      torch.cuda.set_device(gpu)\n",
        "      os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "device = device\n",
        "# args.kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for preprecessing the raw text "
      ],
      "metadata": {
        "id": "uPdnn5sav6DR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEIxVNF_DLvi"
      },
      "source": [
        "def content_gen(preprocessed_text_train):\n",
        "  all_content=[]\n",
        "  for doc in preprocessed_text_train:\n",
        "    words = doc.split()\n",
        "    all_content.append(words)\n",
        "  return all_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##this is prepraed for loading pre-trained embeddings"
      ],
      "metadata": {
        "id": "vqYFlmCmqOQZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4mvEjv6Obqs"
      },
      "source": [
        "embed_size = 200\n",
        " \n",
        "class GloveTokenizer:\n",
        "    def __init__(self, filename, unk='<unk>', pad='<pad>'):  #'<pad>'\n",
        "        self.filename = filename\n",
        "        self.unk = unk\n",
        "        self.pad = pad\n",
        "        self.stoi = dict()\n",
        "        self.itos = dict()\n",
        "        self.embedding_matrix = [] #list()\n",
        "        with open(filename, 'r', encoding='utf8') as f: # Read tokenizer file\n",
        "            for i, line in enumerate(f):\n",
        "                values = line.split()\n",
        "                self.stoi[values[0]] = i\n",
        "                self.itos[i] = values[0]\n",
        "                self.embedding_matrix.append([float(v) for v in values[1:]])\n",
        "        if self.unk is not None: # Add unk token into the tokenizer\n",
        "            i += 1\n",
        "            self.stoi[self.unk] = i\n",
        "            self.itos[i] = self.unk\n",
        "            self.embedding_matrix.append(np.random.rand(embed_size))\n",
        "        if self.pad is not None: # Add pad token into the tokenizer\n",
        "            i += 1\n",
        "            self.stoi[self.pad] = i\n",
        "            self.itos[i] = self.pad\n",
        "            self.embedding_matrix.append(np.zeros(embed_size))\n",
        "\n",
        "        self.embedding_matrix =np.array(self.embedding_matrix) #.astype(np.float32) # Convert if from double to float for efficiency\n",
        "       \n",
        "\n",
        "    def encode(self, sentence):\n",
        "        if type(sentence) == str:\n",
        "            sentence = sentence.split(' ')\n",
        "        elif len(sentence): # Convertible to list\n",
        "            sentence = list(sentence)\n",
        "        else:\n",
        "            raise TypeError('sentence should be either a str or a list of str!')\n",
        "        encoded_sentence = []\n",
        "        for word in sentence:\n",
        "            encoded_sentence.append(self.stoi.get(word, self.stoi[self.unk]))\n",
        "        return encoded_sentence\n",
        "\n",
        "    def decode(self, encoded_sentence):\n",
        "        try:\n",
        "            encoded_sentence = list(encoded_sentence)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            raise TypeError('encoded_sentence should be either a str or a data type that is convertible to list type!')\n",
        "        sentence = []\n",
        "        for encoded_word in encoded_sentence:\n",
        "            sentence.append(self.itos[encoded_word])\n",
        "        return sentence\n",
        "\n",
        "    def embedding(self, encoded_sentence):\n",
        "        return self.embedding_matrix[np.array(encoded_sentence)]\n",
        "\n",
        "\n",
        "tokenizer = GloveTokenizer(f'/content/drive/MyDrive/glove.6B.{embed_size}d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egnHMoMoLWgx"
      },
      "source": [
        "#this is to get total word with freq\n",
        "def build_vocab(train_dataset, tokenizer):\n",
        "    stoi = {'<unk>': 0, '<pad>': 1} # Re-index\n",
        "    itos = {0: '<unk>', 1: '<pad>'} # Re-index\n",
        "    vocab_count = len(stoi)\n",
        "    \n",
        "    vocab_list = [sentence for sentence in train_dataset]\n",
        "    # print(len(vocab_list))\n",
        "    \n",
        "    unique_vocab = []\n",
        "  \n",
        "    for vocab in vocab_list:\n",
        "        unique_vocab.extend(vocab)\n",
        "    \n",
        "    unique_vocab = list(set(unique_vocab))\n",
        "    \n",
        "    # print(len(unique_vocab))\n",
        "    # print(unique_vocab)\n",
        "    for vocab in unique_vocab:\n",
        "        if vocab in tokenizer.stoi.keys():\n",
        "            stoi[vocab] = vocab_count\n",
        "            itos[vocab_count] = vocab\n",
        "            vocab_count += 1\n",
        "    #### ---------get the vocab glove embedding ------#####\n",
        "    # embedding_matrix  = None\n",
        "    embedding_matrix = tokenizer.embedding(tokenizer.encode(list(stoi.keys())))\n",
        "    return vocab_count, stoi, embedding_matrix , itos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eBvpS6YXb-nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is to generate a random embedding for each word"
      ],
      "metadata": {
        "id": "aFTd4DjZqmuJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvH_zkKOgraF"
      },
      "source": [
        "rand_embedding_matrix = np.random.uniform(-1,1, (20000,32))\n",
        "\n",
        "def embed_etract(vocab, embedding_matrix):\n",
        "    feature_matrix = []\n",
        "    \n",
        "    for word_id in vocab:\n",
        "        feature_matrix.append(embedding_matrix[word_id])  ## can also be replaced with glove embeddings as well    \n",
        "    return np.array(feature_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq4pXhfOKH56"
      },
      "source": [
        "\n",
        "def build_nodes(train_doc, stoic): \n",
        "    nn =[sentence for sentence in train_doc]\n",
        "    node_sets= []\n",
        "    node_vob = []\n",
        "    for vocab in nn:\n",
        "        # print(vocab)\n",
        "        node_vob.append(vocab)\n",
        "        node_sets.append(stoic.get(vocab, 0)) #0 represents the position of the value of dict\n",
        "\n",
        "    # print(\"now the train set contains node sets:\", node_sets, len(node_sets))\n",
        "    return node_sets, node_vob  \n",
        "\n",
        "def build_contents(train_doc, stoic, itos): \n",
        "    \n",
        "    content_sets = []\n",
        "    node_vob =[]\n",
        "    nn =[sentence for sentence in train_doc]\n",
        "    for vocab in nn:\n",
        "        node_vob.append(itos.get(vocab))\n",
        "        content_sets.append(stoic.get(vocab, 0)) #0 represents the position of the value of dict\n",
        "    return content_sets , node_vob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graphcon(doc_ids, ngram = 4):\n",
        "    edges = []\n",
        "    old_edge_id = []\n",
        "    local_vocab = np.sort(doc_ids)\n",
        "    old_to_new = dict(zip(local_vocab, range(len(local_vocab))))\n",
        "\n",
        "    for index, src_word_old in enumerate(local_vocab):\n",
        "        src = old_to_new[src_word_old]    \n",
        "        for i in range(max(0, index - ngram), min(index + ngram+1, len(local_vocab))):\n",
        "            dst_word_old = local_vocab[i]\n",
        "            dst = old_to_new[dst_word_old]\n",
        "            edges.append([src, dst])\n",
        "     \n",
        "    edges_n = []\n",
        "    edges_n.extend(edges)\n",
        "        \n",
        "    return edges_n  "
      ],
      "metadata": {
        "id": "1IYFWYF3uWxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_data(all_content, stoic, itos, embed_matrix,  y_label):\n",
        "    \n",
        "    graph_data = []\n",
        "\n",
        "    for doc_id in range(len(all_content)):\n",
        "      \n",
        "        vocab_get= all_content[doc_id]\n",
        "      \n",
        "        node_sets, node_vobs = build_nodes(vocab_get, stoic)\n",
        "        unq_vob = set(node_sets)\n",
        "        \n",
        "        content_sets, nodes = build_contents(unq_vob, stoic, itos)\n",
        "\n",
        "        # print(\"content vocab in each trans\", vocab_get)\n",
        "        # print(\"uniq content lenth in each trans\", len(set(node_sets)), len(node_sets))\n",
        "       \n",
        "        f = embed_etract(unq_vob, embed_matrix)\n",
        "\n",
        "        e  = graphcon (content_sets, ngram = 4) #8\n",
        "        \n",
        "       ##--------------------------------------------------##\n",
        "        edges1 = [np.array([edge[0], edge[1]]) for edge in e]\n",
        "        edge_index = torch.tensor(np.array(edges1).T, dtype=torch.long) #.cuda()\n",
        "      \n",
        "        print(\"edge index size\", edge_index[0].size())\n",
        "        print(\"edge index size2\", edge_index[1].size())\n",
        "        ####-------------------------------###\n",
        "   \n",
        "        ft = torch.tensor(f, dtype=torch.float32) #.cuda()\n",
        "        y  = torch.tensor(y_label[doc_id], dtype= torch.float)\n",
        "        graph_data.append(data.Data(x=ft, edge_index= edge_index, y=y))\n",
        "    \n",
        "    return  graph_data"
      ],
      "metadata": {
        "id": "UYum_6eJVC8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCfR9kHq61nH"
      },
      "source": [
        "\n",
        "content_train = content_gen(preprocessed_text_train)\n",
        "vocab_count,stoic, glove_embedding_matrix, itos  =  build_vocab(content_train, tokenizer)\n",
        "graph_data_list = graph_data(content_train, stoic,itos, rand_embedding_matrix, y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY5UoDdI6_0o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}